# ğŸ” 3. Retrieval & Knowledge Integration (RAG)

RAG (Retrieval-Augmented Generation) enhances LLMs by injecting **external knowledge** into their responses. It overcomes context limitations and hallucinations by combining **retrieval + generation**.

---

## ğŸ§  Why RAG? â€“ The Problem of Limited Context

Large Language Models (LLMs) have **context windows** (e.g., 8kâ€“200k tokens). Beyond that:

- Models **forget** earlier content
- Important info may be **truncated**
- Embedding everything upfront becomes **inefficient and expensive**

### ğŸ”¹ Example
A user asks:
"What did the company say in its June 2023 earnings report?"

You can't feed in the entire financial historyâ€”you must retrieve only the relevant chunk from June 2023.

âœ… Enter RAG: Retrieve only whatâ€™s needed â†’ Inject â†’ Generate.

## ğŸ§¬ Embeddings (for Retrieval)
Embeddings turn text into dense vectors that capture semantic meaning.

ğŸ”¹ Example

| Text                        | Embedding (simplified)              |
| --------------------------- | ----------------------------------- |
| "Apple Inc. profits rose"   | [0.21, 0.98, -0.33, ...]            |
| "Company revenue increased" | [0.22, 0.97, -0.35, ...] âœ… Similar! |


Use embedding models like:
    - OpenAI (text-embedding-3-small)

    - Cohere, HuggingFace (all-MiniLM)

    - BGE, E5, GTE (for multilingual or domain-specific use)

## ğŸ—ƒï¸ Vector Databases

Store and retrieve embeddings efficiently.

| Tool             | Highlights                       |
| ---------------- | -------------------------------- |
| **Pinecone**     | Scalable, managed cloud DB       |
| **Weaviate**     | Semantic + hybrid search         |
| **FAISS**        | Open-source, fast local indexing |
| **Milvus**       | Distributed and performant       |
| **Redis Vector** | Integrates well with Redis stack |

These power similarity search in retrieval pipelines.

## âœ‚ï¸ Chunking Strategies (Text Splitting)

Before embedding, long texts must be chunked.

ğŸ”¹ Common Strategies
| Strategy    | Description                                        | Example                      |
| ----------- | -------------------------------------------------- | ---------------------------- |
| Fixed-size  | Split every N tokens/words                         | 500-token chunks             |
| Recursive   | Split on sections â†’ paragraphs â†’ sentences â†’ words | Keeps structure              |
| Overlapping | Add 10â€“20% overlap between chunks                  | Prevents cutting mid-thought |


ğŸ”¹ Real Example

    An FAQ document is split into:

    Q1 + A1 â†’ Chunk 1

    Q2 + A2 â†’ Chunk 2

    ...

    Each chunk is embedded and stored.


## ğŸ” Retrieval Pipeline
[User Query]
    â†“
[Embedding]
    â†“
[Vector Search (Similarity)]
    â†“
[Top-k Chunks]
    â†“
[Inject into Prompt Template]
    â†“
[LLM Generates Final Answer]


ğŸ”¹ Prompt Template Example
    Answer the question using the context below.

    Context:
    {retrieved_chunk_1}
    {retrieved_chunk_2}

    Question: {user_query}
    Answer:

## âš™ï¸ Hybrid Search (Semantic + Keyword)

Combine:
    - Semantic search: via embeddings
    - Keyword search: via BM25, Elasticsearch, etc.

This improves accuracy when:

    Specific keywords are critical

    Numeric/textual identifiers (e.g., "Model X123") are present

    Many tools (like Weaviate) offer hybrid search out-of-the-box.

## âœ… RAG Evaluation & âŒ Failure Modes
âœ… Evaluation Metrics
    | Metric           | Purpose                                           |
    | ---------------- | ------------------------------------------------- |
    | **Hit rate**     | Was the relevant chunk retrieved?                 |
    | **Faithfulness** | Was the answer grounded in the retrieved context? |
    | **F1 / ROUGE**   | Compare generated vs ground truth (for QA tasks)  |
    | **Human eval**   | Check factuality and helpfulness                  |


## âŒ Failure Cases

    | Issue                  | Description                           | Example                                           |
    | ---------------------- | ------------------------------------- | ------------------------------------------------- |
    | âŒ Bad chunking         | Important info split between chunks   | â€œTerms and conditionsâ€ separated from explanation |
    | âŒ Irrelevant retrieval | Top-k results donâ€™t match query       | Answer refers to wrong product version            |
    | âŒ Context overflow     | Too many chunks â†’ truncation          | LLM drops relevant chunk                          |
    | âŒ Hallucination        | LLM answers outside retrieved context | Answer fabricated beyond provided docs            |


## ğŸ” Real-World Use Cases

1. Customer Support Bot

    - Retrieves product manuals â†’ answers user queries

    - Vector DB: Pinecone

    - Chunking: 300-token + overlap

2. Legal Document Q&A

    - Retrieve relevant case law or clauses

    - Hybrid search: keyword for statute, semantic for meaning

3. Internal Knowledge Assistant

    - Employees ask about internal policies

    - Retrieval from Notion/Confluence â†’ context â†’ GPT-4 answer


### TL;DR

| Concept        | Summary                                             |
| -------------- | --------------------------------------------------- |
| RAG            | Retrieval + Generation to inject external knowledge |
| Embeddings     | Turn text into meaning vectors                      |
| Vector DB      | Stores and retrieves similar chunks                 |
| Chunking       | Splits text efficiently for retrieval               |
| Pipeline       | Embed â†’ Search â†’ Inject â†’ Generate                  |
| Hybrid Search  | Combines keyword and semantic matching              |
| RAG Evaluation | Measures retrieval + output quality                 |
