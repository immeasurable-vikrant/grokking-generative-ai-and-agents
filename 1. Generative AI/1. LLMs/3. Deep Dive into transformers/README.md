Transformers beat RNN/LSTM because:

Parallelization:
RNNs/LSTMs process tokens sequentially, so training is slow.
Transformers use self-attention â†’ process all tokens in parallel â†’ much faster.

Long-Range Dependencies:
RNNs/LSTMs struggle with remembering far-apart words due to vanishing gradients.
Transformersâ€™ self-attention directly connects all tokens to each other, so they capture long-range context better.

Better Scaling:
Transformers scale efficiently with more data and compute â†’ bigger models (like GPT) possible.

Context Flexibility:
Self-attention lets the model focus on important words regardless of position, unlike RNNs that rely on order.

ðŸ‘‰ In short: Faster training + richer context understanding + better scaling â†’ Transformers dominate